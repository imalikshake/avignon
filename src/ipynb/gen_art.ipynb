{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b18427",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de5a64-8b3b-42f9-aeac-0f90d7a8ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import PIL\n",
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"TF Hub version: \", hub.__version__)\n",
    "print(\"Eager mode enabled: \", tf.executing_eagerly())\n",
    "print(\"GPU available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c10ab3-5c71-4a86-8c68-4a986eae94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2c887-a2ce-43e2-889d-bf721b432dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define image loading and visualization functions  { display-mode: \"form\" }\n",
    "\n",
    "def crop_center(image):\n",
    "  \"\"\"Returns a cropped square image.\"\"\"\n",
    "  shape = image.shape\n",
    "  new_shape = min(shape[1], shape[2])\n",
    "  offset_y = max(shape[1] - shape[2], 0) // 2\n",
    "  offset_x = max(shape[2] - shape[1], 0) // 2\n",
    "  image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_y, offset_x, new_shape, new_shape)\n",
    "  return image\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def load_image(image_url, image_size=(256, 256), crop=False, preserve_aspect_ratio=True):\n",
    "  \"\"\"Loads and preprocesses images.\"\"\"\n",
    "  # Cache image file locally.\n",
    "  image_path = image_url\n",
    "  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n",
    "  img = tf.io.decode_image(\n",
    "      tf.io.read_file(image_path),\n",
    "      channels=3, dtype=tf.float32)[tf.newaxis, ...]\n",
    "\n",
    "  if not image_size==0:\n",
    "    img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)\n",
    "  if crop == True:\n",
    "    img = crop_center(img)\n",
    "  return img\n",
    "\n",
    "\n",
    "def show_n(images, titles=('',)):\n",
    "  n = len(images)\n",
    "  image_sizes = [image.shape[1] for image in images]\n",
    "  w = (image_sizes[0] * 6) // 320\n",
    "  plt.figure(figsize=(w * n, w))\n",
    "  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)\n",
    "  for i in range(n):\n",
    "    plt.subplot(gs[i])\n",
    "    plt.imshow(images[i][0], aspect='equal')\n",
    "    plt.axis('off')\n",
    "    plt.title(titles[i] if len(titles) > i else '')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9880ba0-45d1-48dd-92c4-79725499532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
    "hub_module = hub.load(hub_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8a2eb-31d5-425f-b93e-9c28e854e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylise_frames(hub_module, input_frame_path, style_frame_path, content_img_size, style_img_size):\n",
    "    content_image = load_image(input_frame_path, content_img_size)\n",
    "    style_image = load_image(style_frame_path, style_img_size, crop=True)\n",
    "    style_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\n",
    "    outputs = hub_module(tf.constant(content_image), tf.constant(style_image))\n",
    "    stylised_image = outputs[0]\n",
    "    return stylised_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88a077-0909-436c-94c5-82509c49219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensor_image(tensor, output_path):\n",
    "    si_np = tensor.numpy()\n",
    "    si_np = np.squeeze(si_np, axis=0)\n",
    "    si_np = (si_np * 255).astype(np.uint8)\n",
    "    im1 = PIL.Image.fromarray(si_np, 'RGB')\n",
    "    im1.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894560d4-b56c-4ebe-9a4a-9bb635587877",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_url = \"/Users/iman/github/avignon/projects/graphite/graphite_original/0002.png\"\n",
    "style_image_url = \"/Users/iman/github/avignon/projects/graphite/temp/0001.png\"\n",
    "\n",
    "content_img_size = (1920, 1080)\n",
    "style_img_size = (int(1920/4), int(1080/4))  # Recommended to keep it at 256.\n",
    "\n",
    "content_image = load_image(content_image_url, 0)\n",
    "print(content_image.shape)\n",
    "style_image = load_image(style_image_url, style_img_size, crop=False)\n",
    "style_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\n",
    "# show_n([content_image, style_image], ['Content image', 'Style image'])\n",
    "\n",
    "outputs = hub_module(tf.constant(content_image), tf.constant(style_image))\n",
    "stylized_image = outputs[0]\n",
    "\n",
    "show_n([content_image, style_image, stylized_image], titles=['Original content image', 'Style image', 'Stylized image'])\n",
    "save_tensor_image(stylized_image, \"/Users/iman/github/avignon/other/text_out2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22996c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b290d-0e8a-416a-ba20-8a1216be760a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c2662-dc02-4949-88ee-23eef7dfb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/iman/github/avi/test2/keyframes'\n",
    "\n",
    "file_numbers = []\n",
    "\n",
    "# Iterate over the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    filename = os.path.basename(filename)\n",
    "    if 'testimony' in filename:\n",
    "        file_numbers.append(filename.split('testimony')[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72424491-294b-42d1-b83c-8e4bb910e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_imgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea4b5c-1709-4b23-a936-09a9add07049",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in file_numbers:\n",
    "    out_img = stylise_frames(hub_module, input_frame_path=f\"/Users/iman/github/avi/test2/keyframes/testimony{n}.jpg\", style_frame_path=f\"/Users/iman/github/avi/test2/keyframes/testimony{n}.jpg\",content_img_size=(1280, 720),style_img_size=(1280, 720))\n",
    "# show_n([out_img], titles=[\"Result\"])\n",
    "    out_imgs.append(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250fc03-a0d7-45e1-a2b2-924d1caf4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_folder_path = '/Users/iman/github/avi/test2/keyframes_s/'\n",
    "\n",
    "for i, img in enumerate(out_imgs):\n",
    "    n = file_numbers[i]\n",
    "    output_frame_path = style_folder_path+\"testimony\"+n+\".jpg\"\n",
    "    save_tensor_image(img, output_frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2242513-5e28-4156-b393-ceead5e1caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil \n",
    "import PIL, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8bf1d-6411-45c8-bd96-0ba5728ab98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = '/Users/iman/frames_s/'  # Replace with the actual folder path\n",
    "style_folder_path = '/Users/iman/frames_r/'  # Replace with the actual folder path\n",
    "output_folder_path = '/Users/iman/frames_o/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ccb89b-7f17-46cb-812b-23ffd7f57602",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(output_folder_path):\n",
    "    shutil.rmtree(output_folder_path)\n",
    "os.mkdir(output_folder_path)\n",
    "\n",
    "jpeg_files = glob.glob(os.path.join(input_folder_path, '*.jpg'))\n",
    "jpeg_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2b3d1-a4b9-43b0-95c1-b86884a192db",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img_size=(1280, 720)\n",
    "style_img_size=(256*3, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132335c-3186-4e8a-aca0-94b6aa559e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in jpeg_files:\n",
    "    # Perform operations on each JPEG file\n",
    "    print(file_path) \n",
    "    input_frame_path = file_path\n",
    "    style_frame_path = os.path.join(os.path.dirname(style_folder_path), os.path.basename(file_path))\n",
    "    output_frame_path = os.path.join(os.path.dirname(output_folder_path), os.path.basename(file_path))\n",
    "    output_frame = stylise_frames(hub_module, input_frame_path=input_frame_path, style_frame_path=style_frame_path,content_img_size=content_img_size,style_img_size=style_img_size)\n",
    "    # show_n([output_frame], titles=[\"Result\"])\n",
    "    save_tensor_image(output_frame, output_frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738fcb1-22c6-41b0-84b5-c1258a872b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stylise_frames(hub_module, input_frame_path=input_frame_path, style_frame_path=style_frame_path,content_img_size=content_img_size,style_img_size=style_img_size)\n",
    "    save_tensor_image(output_frame, output_frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b554e5f-9fc2-4ad4-9452-648852245bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa64a1-de4d-4cc9-9cb2-36bb5d6e136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read two consecutive frames from the video\n",
    "frame1 = cv2.imread('/Users/iman/github/avignon/projects/graphite/output/0000.png')\n",
    "frame2 = cv2.imread('/Users/iman/github/avignon/projects/graphite/output/0199.png')\n",
    "\n",
    "# Specify the number of new frames to create\n",
    "num_frames = 10\n",
    "\n",
    "# Convert frames to grayscale\n",
    "# frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "# frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# Calculate the interpolation step size\n",
    "interpolation_step = 1 / (num_frames + 1)\n",
    "\n",
    "# Create and save the new frames\n",
    "for i in range(1, num_frames + 1):\n",
    "    # Calculate the interpolation factor\n",
    "    interpolation_factor = i * interpolation_step\n",
    "\n",
    "    # Perform the interpolation\n",
    "    interpolated_frame = cv2.addWeighted(frame1, interpolation_factor, frame2, 1 - interpolation_factor, 0)\n",
    "\n",
    "    # Save the interpolated frame\n",
    "    cv2.imwrite(f'interpolated_frame_{i}.jpg', interpolated_frame)\n",
    "\n",
    "    # Display the interpolated frame\n",
    "    show_image(interpolated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f6860-c799-4d2c-925f-63ffe864a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, fig_size=(10,20)):\n",
    "    image = np.squeeze(image)\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0179c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames_directory = '/Users/iman/github/avignon/projects/graphite/output'\n",
    "output_video_path = '/Users/iman/output_video.mp4'\n",
    "command_buffer = {}\n",
    "\n",
    "input_files = glob.glob(os.path.join(frames_directory, '*.png'))\n",
    "input_files.sort()\n",
    "interpolation_factor = 0.5\n",
    "frame_rate = 270\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter(output_video_path, fourcc, 30, (1080, 1920))  # Assuming frame size is 640x480\n",
    "do_blur = 0\n",
    "# kernel_sizes = [num for num in range(2, 1000) if num % 2 != 0]\n",
    "kernel_sizes = [3,5,7]\n",
    "skip_percentage = 5\n",
    "\n",
    "for i, frame_path in enumerate(input_files):\n",
    "    random_number = random.randint(0,100)\n",
    "    frame_duration = int(1000 / frame_rate)\n",
    "    current_frame = cv2.imread(frame_path) \n",
    "    next_frame =  cv2.imread(input_files[min(len(input_files)-1,i+1)]) \n",
    "    interpolated_frame = cv2.addWeighted(current_frame, interpolation_factor, next_frame, 1 - interpolation_factor, 0)\n",
    "    \n",
    "    if random_number < skip_percentage:\n",
    "        do_blur = do_blur + 10\n",
    "\n",
    "    if do_blur:\n",
    "        kernel_size = (kernel_sizes[min(len(kernel_sizes)-1,do_blur)], kernel_sizes[min(len(kernel_sizes)-1,do_blur)])  # Adjust the kernel size based on the desired blur effect\n",
    "        interpolated_frame = cv2.GaussianBlur(interpolated_frame, kernel_size, 0)\n",
    "        do_blur = do_blur - 1\n",
    "\n",
    "    for _ in range(frame_duration):\n",
    "        output_video.write(interpolated_frame)\n",
    "    \n",
    "    print(do_blur)\n",
    "    \n",
    "# Release the video writer\n",
    "output_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bf147",
   "metadata": {},
   "outputs": [],
   "source": [
    "1000/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b654bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "    interpolated_frame = cv2.GaussianBlur(interpolated_frame, kernel_size, 0)\n",
    "    if random_number < skip_percentage:\n",
    "        frames_skip = random.randint(max(0,i-20), min((i+20),len(input_files)-1))\n",
    "        i = frames_skip\n",
    "        current_frame = interpolated_frame\n",
    "        next_frame = cv2.imread(input_files[i])\n",
    "        interpolated_frame = cv2.addWeighted(current_frame, interpolation_factor, next_frame, 1 - interpolation_factor, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871366b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
